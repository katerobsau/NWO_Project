---
title: "Event Based Verification"
output: html_document
---

```{r loadPackages, echo = FALSE, warning = FALSE, message = FALSE}
library(tidyverse)
library(cowplot)
source("code/figure_labels.R")
# source("code/source/surge_exploratory_analyis.R")
```

<!-- ```{r tidalPackage read, echo = FALSE, warning = FALSE, message = FALSE} -->
<!-- package_dir = "code/tidalHelpers" -->
<!-- install.packages(pkgs = "code/tidalHelpers", repos = NULL, type = "source") -->
<!-- library(tidalHelpers) -->
<!-- ``` -->

In contrast to standard post-processing, we are interested in specific features of the trajectories that are timing and duration dependent. Consider the following examples:

* When does the storm surge peak?

$$g(\mathbf{s}_t; k) = \displaystyle{\text{argmax}_{k \in K}} \left( S_{t+k} \right)$$

* What is the maximum height of the storm surge?

$$g(\mathbf{s}_t; k) = \displaystyle{\text{max}_{k \in K}} \left( S_{t+k} \right)$$

* How long does the storm surge last?

$$g(\mathbf{s}_t; k) = \tfrac{1}{K\Delta{k}}\sum_{k \in K} \mathbb{I}\left(S_{t+k} > 0 \right)$$

* How long is the total water level above a given risk threshold,  $r$?

$$g(\mathbf{s}_t; k, r) = \tfrac{1}{K\Delta{k}}\sum_{k \in K} \mathbb{I}\left(S_{t+k} + h_{t+k} > r \right)$$

These quantitites can empircally be estimated from the raw ensemble:

$$ P_t \left[g(\mathbf{s}_t, \theta)\right] = \tfrac{1}{M} \sum_{m=1}^M g(\mathbf{s}^{(m)}_t, \theta).$$
Pinson et al. 2012 refers to this as event-based approach. The questions is now whether these scenario based metrics can be used to optimise parameter estimates during post-processing, opposed to evaluation after the fact. Of note is that assessing trajectories with standard skill metrics, like the Brier Skill Score, requires taking this autocorrelation and aggreation into account. We also must adjust for the increasing variability at longer lead times. 

[^1]: 
Careful with notion here for threshold height
